{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/chaeyun-jang/.conda/envs/plms/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import re \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import Rbeast\n",
    "from tqdm import tqdm\n",
    "from src.utils import convert_to_llama_prompt, read_json\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    padding_side=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "def determine_answer(sentence):\n",
    "    options = ['A', 'B', 'C', 'D', 'E']\n",
    "    last_option = None\n",
    "\n",
    "    for word in sentence:\n",
    "        if word in options:\n",
    "            last_option = word\n",
    "\n",
    "    return last_option\n",
    "\n",
    "\n",
    "def one_hot_encode(option):\n",
    "    options = ['A', 'B', 'C', 'D', 'E']\n",
    "    encoding = np.zeros(len(options))\n",
    "    if option in options:\n",
    "        encoding[options.index(option)] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "data_paths = os.listdir('./src/data/original_data')\n",
    "datas = [pd.read_csv(os.path.join('./src/data/original_data', p)) for p in data_paths]\n",
    "\n",
    "processed_datas = []\n",
    "for data in datas:\n",
    "    new_data = data.copy()\n",
    "    new_data['input'] = convert_to_llama_prompt(data['input'])\n",
    "    processed_datas.append(new_data)\n",
    "\n",
    "input_prompts = list(processed_datas[0]['input'])\n",
    "answers = list(processed_datas[0]['ground_truth'])\n",
    "\n",
    "# base_path + top_k replace\n",
    "all_data_path = [os.path.join('./src/data/base_path', p) for p in os.listdir('./src/data/base_path')]\n",
    "all_data = []\n",
    "for p in all_data_path:\n",
    "    all_data.append(read_json(p))\n",
    "\n",
    "# resampling \n",
    "forking_data_path = [os.path.join('./src/data/forking_path/aqua', p) for p in os.listdir('./src/data/forking_path/aqua')]\n",
    "forking_data = []\n",
    "forking_data_index = []\n",
    "\n",
    "for p in forking_data_path:\n",
    "    forking_data.append(read_json(p))\n",
    "    forking_data_index.append(int(re.sub(r'[^0-9]', '', p.split('aqua')[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make outcome distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dist_o_t_w = []\n",
    "list_dist_o_t = []\n",
    "\n",
    "list_base_path = []\n",
    "\n",
    "list_answer = []\n",
    "list_answer_score = []\n",
    "\n",
    "ground_truth_answer = []\n",
    "\n",
    "for e_i, index in enumerate(forking_data_index):\n",
    "    \n",
    "    all_o_t_w = [] \n",
    "    all_o_t_w_s =[]\n",
    "    all_o_t_w_prob = []\n",
    "\n",
    "    for t_s, idx, t_prob in zip(forking_data[e_i]['all_path'], forking_data[e_i]['t_index'], forking_data[e_i]['prob']):\n",
    "        \n",
    "        temp_o_t = []\n",
    "        temp_o_t_s = []\n",
    "        temp_o_t_prob = []\n",
    "        \n",
    "        for i in range(int(len(t_s)//30)):\n",
    "            \n",
    "            temp_o_t_w = []\n",
    "            o_t_w_prob = []\n",
    "            for j in range(30):\n",
    "                answer = determine_answer(t_s[i*30+j])\n",
    "                encoded_answer = one_hot_encode(answer)\n",
    "                temp_o_t_w.append(encoded_answer)\n",
    "                o_t_w_prob.append(t_prob[i*30+j])\n",
    "                \n",
    "            temp_o_t_s.append(all_data[0]['replace_token_score'][index][idx][i])\n",
    "            temp_o_t.append(temp_o_t_w)\n",
    "            temp_o_t_prob.append(o_t_w_prob)\n",
    "        \n",
    "        all_o_t_w.append(temp_o_t)\n",
    "        all_o_t_w_s.append(temp_o_t_s)\n",
    "        all_o_t_w_prob.append(temp_o_t_prob)\n",
    "            \n",
    "    new = []\n",
    "    for i in range(len(all_o_t_w_prob[0][0])):\n",
    "        new.append(all_o_t_w_prob[0][0][i] * all_o_t_w[0][0][i])\n",
    "\n",
    "    dist_o_t_w = []\n",
    "    dist_o_t = []\n",
    "\n",
    "    for o_t_w, o_t_w_prob in zip(all_o_t_w, all_o_t_w_prob): \n",
    "        temp_dist_o_t_w = []\n",
    "        for t_w, t_w_prob in zip(o_t_w, o_t_w_prob):\n",
    "            new = []\n",
    "            for i in range(len(t_w)):\n",
    "                new.append(t_w_prob[i] * t_w[i])\n",
    "            temp_dist_o_t_w.append(np.mean(new, axis=0))\n",
    "        dist_o_t_w.append(temp_dist_o_t_w)\n",
    "\n",
    "    for o_t_w, o_t_w_s in zip(dist_o_t_w, all_o_t_w_s):\n",
    "        new = []\n",
    "        for i in range(len(o_t_w)):\n",
    "            new.append(o_t_w[i] * o_t_w_s[i])\n",
    "        dist_o_t.append(np.mean(new, axis=0))\n",
    "    \n",
    "    list_dist_o_t_w.append(dist_o_t_w)\n",
    "    list_dist_o_t.append(dist_o_t)\n",
    "    \n",
    "    list_base_path.append(all_data[0]['base_path'][index])\n",
    "    list_answer.append(all_data[0]['replace_token'][index])\n",
    "    list_answer_score.append(all_data[0]['replace_token_score'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Bayesian Change Point Detection (BCPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_y(list_dist_o_t):\n",
    "    o_0 = np.array(list_dist_o_t[0][0])\n",
    "\n",
    "    y = []\n",
    "    time = []\n",
    "\n",
    "    for t, o_t in enumerate(list_dist_o_t):\n",
    "        y.append(np.linalg.norm(o_0 - np.array(o_t)))  \n",
    "        time.append(t)  \n",
    "\n",
    "    return np.array(y), np.array(time)\n",
    "\n",
    "\n",
    "def run_beast_for_cpd(y, time, alpha2_str='lambda range_y: 2.0 + (1000 ** (1.0 - range_y))',\n",
    "                      tcp_minmax=[0, 6], tseg_minlength=10, mcmc_chains=10, mcmc_burnin=1000, \n",
    "                      mcmc_samples=20000, mcmc_thin=5, prec_value=10, alpha1=0.01):\n",
    "    \n",
    "    alpha2_fn = eval(alpha2_str)\n",
    "\n",
    "    range_y = y.max() - y.min()\n",
    "    alpha2 = alpha2_fn(range_y)\n",
    "\n",
    "    result = Rbeast.beast(\n",
    "        y,\n",
    "        time=time,\n",
    "        season='none',\n",
    "        tcp_minmax=tcp_minmax,\n",
    "        torder_minmax=[1, 1],\n",
    "        tseg_minlength=tseg_minlength,\n",
    "        mcmc_seed=0,\n",
    "        mcmc_chains=mcmc_chains,\n",
    "        mcmc_burnin=mcmc_burnin,\n",
    "        mcmc_samples=mcmc_samples,\n",
    "        mcmc_thin=mcmc_thin,\n",
    "        print_progress=False,\n",
    "        print_options=False,\n",
    "        quiet=True,\n",
    "        precPriorType='constant',\n",
    "        precValue=prec_value,\n",
    "        alpha1=alpha1,\n",
    "        alpha2=alpha2\n",
    "    )\n",
    "\n",
    "    return result.trend.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_answers = ['A', 'B', 'C', 'D', 'E', ' A', ' B', ' C', ' D', 'E', '(A', '(B', '(C', '(D', '(E']\n",
    "available_answers_index = [tokenizer.encode(t, add_special_tokens=False)[0] for t in available_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer_logit(list_base_path, list_answer_score):\n",
    "    all_score = []\n",
    "    answer_list =[]\n",
    "    for j, sent in enumerate(list_base_path):\n",
    "        answer = 'None'\n",
    "        last_answer_index = -1\n",
    "        for i, token in enumerate(sent):\n",
    "            if token in available_answers_index:\n",
    "                last_answer_index = i \n",
    "                answer = tokenizer.decode(token).strip()\n",
    "                if '(' in answer:\n",
    "                    answer = answer.repace('(', '')\n",
    "        \n",
    "        s = list_answer_score[j][last_answer_index]\n",
    "        all_score.append(s)\n",
    "        answer_list.append(answer)\n",
    "    return all_score, answer_list\n",
    "\n",
    "all_score, answer_list = find_answer_logit(list_base_path, list_answer_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_token_index = []\n",
    "\n",
    "critical_token_score = []\n",
    "last_answer_token_score = []\n",
    "\n",
    "ground_truth_answer = []\n",
    "\n",
    "for i in range(len(list_dist_o_t)):\n",
    "    y, time = preprocess_y(list_dist_o_t[i])\n",
    "    cp = run_beast_for_cpd(y=y, time=time)['cp'][0]\n",
    "    #print(tokenizer.decode(list_base_path[i][forking_data[i]['t_index'][int(cp)]]))\n",
    "    critical_token_score.append(np.max(list_answer_score[i][forking_data[i]['t_index'][int(cp)]]))\n",
    "    last_answer_token_score.append(np.max(all_score[i]))\n",
    "    ground_truth_answer.append(int(answers[i]==answer_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['critical_token_score'] = critical_token_score\n",
    "df['ground_truth_answer'] = ground_truth_answer\n",
    "df['last_answer_token_score'] = last_answer_token_score\n",
    "df['answer_list']   = answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4633710563182831, 0.7258704992135366)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_ece(scores, ground_truth, num_bins=10):\n",
    "    bins = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_indices = np.digitize(scores, bins, right=True)\n",
    "    \n",
    "    ece = 0\n",
    "    for bin_lower in range(1, num_bins + 1):\n",
    "        bin_scores = scores[bin_indices == bin_lower]\n",
    "        bin_truth = ground_truth[bin_indices == bin_lower]\n",
    "        if len(bin_scores) > 0:\n",
    "            bin_accuracy = np.mean(bin_truth)\n",
    "            bin_confidence = np.mean(bin_scores)\n",
    "            ece += (len(bin_scores) / len(scores)) * abs(bin_accuracy - bin_confidence)\n",
    "    return ece\n",
    "\n",
    "critical_scores = df[\"critical_token_score\"].values\n",
    "last_scores = df[\"last_answer_token_score\"].values\n",
    "ground_truth = df[\"ground_truth_answer\"].values\n",
    "\n",
    "critical_ece = calculate_ece(critical_scores, ground_truth)\n",
    "answer_token_ece = calculate_ece(last_scores, ground_truth)\n",
    "\n",
    "critical_ece, answer_token_ece"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
